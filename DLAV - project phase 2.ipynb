{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg5EMWEn5zmH"
      },
      "source": [
        "# Phase 2: Trajectory Prediction with Auxiliary Depth Estimation\n",
        "\n",
        "# ðŸ§­ Introduction\n",
        "\n",
        "\"\"\"\n",
        "Welcome to **Phase 2** of the DLAV Projec! ðŸš—ðŸ’¨\n",
        "\n",
        "In this phase, you'll work with a more challenging dataset that includes:\n",
        "- RGB **camera images**\n",
        "- Ground-truth **depth maps**\n",
        "- Ground-truth **semantic segmentation** labels\n",
        "\n",
        "Your goal is still to predict the **future trajectory** of the self-driving car (SDC), but you now have more tools at your disposal! ðŸŽ¯\n",
        "\n",
        "Here, we provide an example where **depth estimation** is used as an auxiliary task to improve trajectory prediction.\n",
        "\n",
        "However, you're **free to explore** other auxiliary tasks (e.g., using semantic labels), different loss functions, data augmentations, or better architectures! ðŸ’¡\n",
        "\n",
        "This notebook will walk you through loading the dataset, building a model, training with and without the auxiliary task, and visualizing results.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ulknsOA5x7-",
        "outputId": "0bca5966-df08-487c-fa27-cc4e53410854"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1YkGwaxBKNiYL2nq--cB6WMmYGzRmRKVr\n",
            "From (redirected): https://drive.google.com/uc?id=1YkGwaxBKNiYL2nq--cB6WMmYGzRmRKVr&confirm=t&uuid=ecb60021-fd3d-4ac4-bba8-389fab1397f1\n",
            "To: /content/dlav_train.zip\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 439M/439M [00:09<00:00, 45.8MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1wtmT_vH9mMUNOwrNOMFP6WFw6e8rbOdu\n",
            "From (redirected): https://drive.google.com/uc?id=1wtmT_vH9mMUNOwrNOMFP6WFw6e8rbOdu&confirm=t&uuid=efa1314a-c3a1-4c58-8880-ec29c1fb1128\n",
            "To: /content/dlav_val.zip\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87.8M/87.8M [00:02<00:00, 35.8MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1G9xGE7s-Ikvvc2-LZTUyuzhWAlNdLTLV\n",
            "From (redirected): https://drive.google.com/uc?id=1G9xGE7s-Ikvvc2-LZTUyuzhWAlNdLTLV&confirm=t&uuid=7c2fde8e-756b-4fb2-8caf-bb846d15cca5\n",
            "To: /content/dlav_test_public.zip\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 86.6M/86.6M [00:02<00:00, 37.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Install gdown to handle Google Drive file download\n",
        "!pip install -q gdown\n",
        "\n",
        "import gdown\n",
        "import zipfile\n",
        "\n",
        "download_url = f\"https://drive.google.com/uc?id=1YkGwaxBKNiYL2nq--cB6WMmYGzRmRKVr\"\n",
        "output_zip = \"dlav_train.zip\"\n",
        "gdown.download(download_url, output_zip, quiet=False)  # Downloads the file to your drive\n",
        "with zipfile.ZipFile(output_zip, 'r') as zip_ref:  # Extracts the downloaded zip file\n",
        "    zip_ref.extractall(\".\")\n",
        "\n",
        "download_url = \"https://drive.google.com/uc?id=1wtmT_vH9mMUNOwrNOMFP6WFw6e8rbOdu\"\n",
        "output_zip = \"dlav_val.zip\"\n",
        "gdown.download(download_url, output_zip, quiet=False)\n",
        "with zipfile.ZipFile(output_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\".\")\n",
        "\n",
        "download_url = \"https://drive.google.com/uc?id=1G9xGE7s-Ikvvc2-LZTUyuzhWAlNdLTLV\"\n",
        "output_zip = \"dlav_test_public.zip\"\n",
        "gdown.download(download_url, output_zip, quiet=False)\n",
        "with zipfile.ZipFile(output_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\".\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Various imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import pickle\n",
        "import os\n",
        "import copy\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from google.colab import drive\n",
        "from torchvision import transforms\n",
        "from torchvision import models\n",
        "from torchvision.models import ResNet18_Weights\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TptnDCxT7ATM"
      },
      "source": [
        "## ðŸ“‚ The Dataset\n",
        "\n",
        "We are now working with a richer dataset that includes not just images and trajectories,\n",
        "but also **depth maps** (and semantic segmentation labels, though unused in this example).\n",
        "\n",
        "The data is stored in `.pkl` files and each file contains:\n",
        "- `camera`: RGB image (shape: H x W x 3)\n",
        "- `sdc_history_feature`: the past trajectory of the car\n",
        "- `sdc_future_feature`: the future trajectory to predict\n",
        "- `depth`: ground truth depth map (shape: H x W x 1)\n",
        "\n",
        "We'll define a `DrivingDataset` class to load and return these tensors in a format our model can work with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZGKlrP86QtM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import csv\n",
        "import random\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "class DrivingDataset(Dataset):\n",
        "    def __init__(self, file_list, test=False,val=False,augment=False):\n",
        "        self.samples = file_list\n",
        "        self.test = test\n",
        "        self.val = val\n",
        "        self.augment=augment\n",
        "\n",
        "        self.transform1 = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),  # Resize to 224x224\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load pickle file\n",
        "        with open(self.samples[idx], 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "\n",
        "        # Convert numpy arrays to tensors\n",
        "        camera = torch.FloatTensor(data['camera']).permute(2, 0, 1)\n",
        "        camera = self.transform1(camera)\n",
        "        history = torch.FloatTensor(data['sdc_history_feature'])\n",
        "        command = data['driving_command']\n",
        "        command_map = {'left':0,'right':1,'forward': 2}\n",
        "        command = torch.tensor(command_map[command])\n",
        "        depth = torch.FloatTensor(data['depth'])\n",
        "        depth=depth.permute(2,0,1)\n",
        "\n",
        "        if not self.test:\n",
        "          future = torch.FloatTensor(data['sdc_future_feature'])\n",
        "\n",
        "        if self.augment and (random.random() < 0.5) and (not self.test):\n",
        "          camera = torch.flip(camera, dims=[2])\n",
        "          history[0,:] = -history[0,:]\n",
        "          future[0,:] = -future[0,:]\n",
        "\n",
        "        if not self.test:\n",
        "          return {\n",
        "            'camera': camera,\n",
        "            'history': history,\n",
        "            'command': command,\n",
        "            'future': future,\n",
        "            'depth': depth\n",
        "          }\n",
        "        else:\n",
        "          return {\n",
        "            'camera': camera,\n",
        "            'history': history,\n",
        "            'command': command,\n",
        "            'depth': depth\n",
        "          }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DrivingDataset2(Dataset):\n",
        "    \"\"\"\n",
        "    Data loader with additional data processing\n",
        "    \"\"\"\n",
        "    def __init__(self, file_list, test=False,val=False,augment=False):\n",
        "        self.samples = file_list\n",
        "        self.test = test\n",
        "        self.val = val\n",
        "        self.augment=augment\n",
        "\n",
        "        self.transform1 = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),  # Resize to 224x224\n",
        "        ])\n",
        "\n",
        "        # Image transform\n",
        "        self.img_transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.3, saturation=0.2, hue=0.1) if augment else transforms.Lambda(lambda x: x),\n",
        "            transforms.RandomRotation(degrees=10) if augment else transforms.Lambda(lambda x: x),\n",
        "            transforms.RandomResizedCrop(size=(224, 224), scale=(0.9, 1.1)) if augment else transforms.Lambda(lambda x: x),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        # Depth transform\n",
        "        self.depth_transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.Lambda(lambda x: x / x.max())\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load pickle file\n",
        "        with open(self.samples[idx], 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "\n",
        "        # Convert numpy arrays to tensors (/255?)\n",
        "        camera = torch.FloatTensor(data['camera']).permute(2, 0, 1)\n",
        "        camera = self.img_transform(camera)\n",
        "\n",
        "        depth = torch.FloatTensor(data['depth']).permute(2, 0, 1)\n",
        "        depth = self.depth_transform(depth)\n",
        "\n",
        "        # Optional depth augmentation\n",
        "        if self.augment and not self.test:\n",
        "            if random.random() < 0.3:\n",
        "                depth *= torch.rand(1) * 0.2 + 0.9\n",
        "            if random.random() < 0.3:\n",
        "                depth += torch.randn_like(depth) * 0.01\n",
        "\n",
        "        history = torch.FloatTensor(data['sdc_history_feature'])\n",
        "        history = (history - history.mean()) / (history.std() + 1e-8)\n",
        "\n",
        "        command = data['driving_command']\n",
        "        command_map = {'left':0,'right':1,'forward': 2}\n",
        "        command = torch.tensor(command_map[command])\n",
        "\n",
        "        if not self.test:\n",
        "          future = torch.FloatTensor(data['sdc_future_feature'])\n",
        "          future = (future - future.mean()) / (future.std() + 1e-8)\n",
        "\n",
        "        # --- Data Augmentation ---\n",
        "        if self.augment and not self.test:\n",
        "            # 1. Random flip (x & yaw)\n",
        "            if random.random() < 0.5:\n",
        "                camera = torch.flip(camera, dims=[2])\n",
        "                depth = torch.flip(depth, dims=[2])\n",
        "                history[:, 0] = -history[:, 0]\n",
        "                history[:, 2] = -history[:, 2]\n",
        "                if future is not None:\n",
        "                    future[:, 0] = -future[:, 0]\n",
        "                    future[:, 2] = -future[:, 2]\n",
        "\n",
        "            # 2. Smooth trajectory (x & y)\n",
        "            if random.random() < 0.5:\n",
        "                kernel = torch.ones(1, 1, 5) / 5.0\n",
        "                for i in range(2):\n",
        "                    history[:, i] = F.conv1d(history[:, i].unsqueeze(0).unsqueeze(0),\n",
        "                                             kernel.to(history.device),\n",
        "                                             padding=2).squeeze(0).squeeze(0)\n",
        "\n",
        "            # 3. Speed perturbation (x, y)\n",
        "            if random.random() < 0.3:\n",
        "                speed_noise = torch.randn(history[:, :2].shape) * 0.05\n",
        "                history[:, :2] += speed_noise\n",
        "\n",
        "            # 4. Yaw jitter\n",
        "            if random.random() < 0.3:\n",
        "                yaw_noise = torch.randn(history[:, 2].shape) * 0.1\n",
        "                history[:, 2] += yaw_noise\n",
        "\n",
        "\n",
        "        if not self.test:\n",
        "          return {\n",
        "            'camera': camera,\n",
        "            'history': history,\n",
        "            'command': command,\n",
        "            'future': future,\n",
        "            'depth': depth\n",
        "          }\n",
        "        else:\n",
        "          return {\n",
        "            'camera': camera,\n",
        "            'history': history,\n",
        "            'command': command,\n",
        "            'depth': depth\n",
        "          }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Logger:\n",
        "    # In this case, the logger was not used\n",
        "    def __init__(self):\n",
        "        # Placeholder for potential future configs (e.g., log_dir, wandb_enabled, etc.)\n",
        "        pass\n",
        "\n",
        "    def log(self, step=None, **metrics):\n",
        "        \"\"\"\n",
        "        Logs the given metrics.\n",
        "\n",
        "        Args:\n",
        "            step (int, optional): The current step or epoch. Useful for tracking.\n",
        "            **metrics: Arbitrary keyword arguments representing metric names and values.\n",
        "        \"\"\"\n",
        "        prefix = f\"[Step {step}] \" if step is not None else \"\"\n",
        "        metric_str = \" | \".join(f\"{k}: {v}\" for k, v in metrics.items())\n",
        "        # print(prefix + metric_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Architecture modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CommandEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Pipeline for the driving_command input.\n",
        "    Simple learnable embedding layer.\n",
        "    Per batch;  string input\n",
        "                [1xembed_dim] tensor output\n",
        "    \"\"\"\n",
        "    def __init__(self,embed_dim=32):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_embeddings=3, embedding_dim=embed_dim)\n",
        "\n",
        "    def forward(self, command):\n",
        "        return self.embedding(command)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CameraEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Pipeline for the camera input.\n",
        "    Use ResNet18 architecture, 18-layer deep CNN. First weights are freezed.\n",
        "    Per batch;  [224x224x3] tensor RGB camera input\n",
        "                [1xoutput_dim] tensor output\n",
        "    \"\"\"\n",
        "    def __init__(self,output_dim=256):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load pretrained ResNet18, freeze first layer and modify last to match dimensions\n",
        "        resnet = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "        for name, param in resnet.named_parameters():\n",
        "            if name.startswith(\"conv1\") or name.startswith(\"bn1\") or name.startswith(\"layer1\"):\n",
        "                param.requires_grad = False\n",
        "\n",
        "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "        self.fc = nn.Linear(512,output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        features = features.view(x.size(0), -1)  # [B,512]\n",
        "        out = self.fc(features)               # [B,output_dim]\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CameraEncoder2(nn.Module):\n",
        "    \"\"\"\n",
        "    Pipeline for the camera input.\n",
        "    Use ResNet18 architecture, 18-layer deep CNN. First weights are freezed.\n",
        "    Per batch;  [224x224x3] tensor RGB camera input\n",
        "                [1xoutput_dim] tensor before and after flattening output\n",
        "    \"\"\"\n",
        "    def __init__(self,output_dim=256):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load pretrained ResNet18, freeze first layer and modify last to match dimensions\n",
        "        resnet = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "        for name, param in resnet.named_parameters():\n",
        "            if name.startswith(\"conv1\") or name.startswith(\"bn1\") or name.startswith(\"layer1\"):\n",
        "                param.requires_grad = False\n",
        "\n",
        "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "        self.fc = nn.Linear(512,output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features1 = self.backbone(x)\n",
        "        features = features1.view(x.size(0), -1)  # [B,512]\n",
        "        out = self.fc(features)               # [B,output_dim]\n",
        "        return out,features1    # extract features before flattening"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HistoryEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Pipeline for the sdc_history_feature input.\n",
        "    Use Transformer blocks, output the last timestep.\n",
        "    Per batch;  [21x3] tensor position history input\n",
        "                [1xd_model] tensor output\n",
        "    \"\"\"\n",
        "    def __init__(self,d_model=128,nhead=4,num_layers=2,dropout=0.1,seq_len=21):\n",
        "        super().__init__()\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(seq_len, d_model))  # Learned positional embedding\n",
        "        self.input_proj = nn.Linear(3, d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model,\n",
        "                                                   nhead,\n",
        "                                                   dim_feedforward=4*d_model,\n",
        "                                                   dropout=dropout)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "\n",
        "    def forward(self, history):\n",
        "        x = self.input_proj(history)  # [B,21,d_model]\n",
        "        x = x + self.pos_embedding\n",
        "        x = x.permute(1, 0, 2)\n",
        "        out = self.transformer(x)\n",
        "        return out[-1]  # last timestep; [B,d_model]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DepthEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Pipeline for the depth map input.\n",
        "    Use several layers of CNNs.\n",
        "    Per batch;  [1x200x300] depth map tensor input.\n",
        "                [1xoutput_dim] tensor output.\n",
        "    \"\"\"\n",
        "    def __init__(self, output_dim=256):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=5, stride=2, padding=2),  # -> (B, 16, 100, 150)\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),  # -> (B, 32, 50, 75)\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # -> (B, 64, 25, 38)\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.AdaptiveAvgPool2d((1, 1))  # -> (B, 64, 1, 1)\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(64, output_dim)  # â†’ (B, output_dim)\n",
        "\n",
        "    def forward(self, x):  # x shape: (B, 1, 200, 300)\n",
        "        x = self.encoder(x)        # -> (B, 64, 1, 1)\n",
        "        x = x.view(x.size(0), -1)  # -> (B, 64)\n",
        "        x = self.fc(x)             # -> (B, output_dim)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DepthDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Predicts the depth map from the RGB processing for the auxiliary loss calculation.\n",
        "    Use several layers of CNN.\n",
        "    Per batch;  [input_channelsxinput_channels] processed RGB input tensor (features extracted by CameraEncoder)\n",
        "                [200,300] prediction of the depth map output\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels=512, output_size=(200, 300)):\n",
        "        super().__init__()\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(input_channels, 256, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(256, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 1, kernel_size=3, padding=1),\n",
        "            nn.Upsample(size=output_size, mode='bilinear', align_corners=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decoder(x)  # Output shape: (B, 1, H, W)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Decode the 3 input pipeline results.\n",
        "    Use concatenation and fully connected layer.\n",
        "    Per batch;  processed camera, history and command input (respectively [1,camera_dim], [1,history_dim], [1,command_dim])\n",
        "                [60,3] future position predictions\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=416, output_dim=3, sequence_length=60):\n",
        "        super(SimpleDecoder, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, output_dim*sequence_length)\n",
        "\n",
        "    def forward(self, camera_feat, history_feat, command_feat):\n",
        "\n",
        "        combined_feat = torch.cat([camera_feat, history_feat, command_feat], dim=-1)  # [B,input_dim]\n",
        "        future_positions_flat = self.fc(combined_feat)  # [B,60*3]\n",
        "        future_positions = future_positions_flat.view(-1,60,3)\n",
        "\n",
        "        return future_positions  # [B,60,3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AttentionDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Decode the 3 input pipeline results.\n",
        "    Use cross-attention blocks.\n",
        "    Per batch;  processed camera, history and command input (respectively [1,camera_dim], [1,history_dim], [1,command_dim])\n",
        "                [60,3] future position predictions\n",
        "    \"\"\"\n",
        "    def __init__(self, output_dim=3, sequence_length=60,d_latent=128,d_camera=256):\n",
        "        super().__init__()\n",
        "        self.d_latent=d_latent\n",
        "\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_latent, nhead=4, batch_first=True)\n",
        "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n",
        "        self.fc = nn.Linear(d_latent, output_dim*sequence_length)\n",
        "\n",
        "        self.proj_cam = nn.Linear(d_camera, self.d_latent)\n",
        "        self.proj_hist = nn.Linear(128,self.d_latent)\n",
        "        self.proj_com = nn.Linear(32,self.d_latent)\n",
        "\n",
        "    def forward(self, camera_feat, history_feat, command_feat):\n",
        "\n",
        "        camera_feat=self.proj_cam(camera_feat)\n",
        "        history_feat=self.proj_hist(history_feat)\n",
        "        command_feat=self.proj_com(command_feat)\n",
        "\n",
        "        x = torch.stack([camera_feat, history_feat, command_feat], dim=1)\n",
        "        attn_out = self.encoder(x)  # [B,3,d_model]\n",
        "        fused_feat = attn_out.mean(dim=1)  # [B,d_model]\n",
        "        predictions = self.fc(fused_feat)\n",
        "        future_pos = predictions.view(-1, 60, 3)\n",
        "\n",
        "\n",
        "        return future_pos  # [B, 60, 3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AttentionDecoderV2(nn.Module):\n",
        "    \"\"\"\n",
        "    Decode the 4 input pipeline results.\n",
        "    Use cross-attention blocks.\n",
        "    Per batch;  processed camera, history and command input (respectively [1,camera_dim], [1,history_dim], [1,command_dim])\n",
        "                [60,3] future position predictions\n",
        "    \"\"\"\n",
        "    def __init__(self, output_dim=3, sequence_length=60,d_latent=128,dim_camera=256):\n",
        "        super().__init__()\n",
        "        self.d_latent=d_latent\n",
        "\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_latent, nhead=4, batch_first=True)\n",
        "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n",
        "        self.fc = nn.Linear(d_latent, output_dim*sequence_length)\n",
        "\n",
        "        self.proj_cam = nn.Linear(dim_camera, self.d_latent)\n",
        "        self.proj_hist = nn.Linear(128,self.d_latent)\n",
        "        self.proj_com = nn.Linear(32,self.d_latent)\n",
        "        self.proj_dep = nn.Linear(256,d_latent)\n",
        "\n",
        "    def forward(self, camera_feat, history_feat, command_feat, depth_feat):\n",
        "\n",
        "        camera_feat=self.proj_cam(camera_feat)\n",
        "        history_feat=self.proj_hist(history_feat)\n",
        "        command_feat=self.proj_com(command_feat)\n",
        "        depth_feat=self.proj_dep(depth_feat)\n",
        "\n",
        "        x = torch.stack([camera_feat, history_feat, command_feat, depth_feat], dim=1)\n",
        "        attn_out = self.encoder(x)  # [B,4,d_model]\n",
        "        fused_feat = attn_out.mean(dim=1)  # [B,d_model]\n",
        "        predictions = self.fc(fused_feat)\n",
        "        future_pos = predictions.view(-1, 60, 3)\n",
        "\n",
        "\n",
        "        return future_pos  # [B, 60, 3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DrivingPlanner(nn.Module):\n",
        "    \"\"\"\n",
        "    End-to-end planner, combine all previously defined modules.\n",
        "    When initialized, input the 3 or 4 features latent dimensions, dropout rate and decoder version.\n",
        "    \"\"\"\n",
        "    def __init__(self,camera_dim,history_dim,command_dim,dropout,version,d_latent=128):\n",
        "        super().__init__()\n",
        "\n",
        "        self.version=version\n",
        "\n",
        "        self.camera_encoder=CameraEncoder(output_dim=camera_dim)\n",
        "        self.history_encoder=HistoryEncoder(d_model=history_dim,dropout=dropout)\n",
        "        self.command_encoder=CommandEncoder(embed_dim=command_dim)\n",
        "\n",
        "        if version==1:\n",
        "          self.decoder=SimpleDecoder()\n",
        "        elif version==2:\n",
        "            self.decoder=AttentionDecoder()\n",
        "        elif version==3:\n",
        "            self.decoder=AttentionDecoderV2(dim_camera=camera_dim,d_latent=d_latent)\n",
        "            self.depth_encoder=DepthEncoder()\n",
        "        elif version==4:\n",
        "            self.decoder=AttentionDecoder(d_camera=camera_dim)\n",
        "            self.aux_decoder=DepthDecoder()\n",
        "            self.camera_encoder=CameraEncoder2(output_dim=camera_dim)\n",
        "\n",
        "    def forward(self, camera, history, command, depth):\n",
        "\n",
        "        camera_feat = self.camera_encoder(camera)\n",
        "        history_feat = self.history_encoder(history)\n",
        "        command_feat = self.command_encoder(command)\n",
        "\n",
        "        if self.version==3:\n",
        "          depth_feat=self.depth_encoder(depth)\n",
        "          future = self.decoder(camera_feat, history_feat, command_feat, depth_feat)\n",
        "          return future\n",
        "\n",
        "        # Combine features\n",
        "        elif self.version==4:\n",
        "          untouched=camera_feat[1]\n",
        "          flatten=camera_feat[0]\n",
        "          future = self.decoder(flatten, history_feat, command_feat)\n",
        "          depth_pred=self.aux_decoder(untouched)\n",
        "          return future,depth_pred\n",
        "\n",
        "\n",
        "        else:\n",
        "          future=self.decoder(camera_feat,history_feat,command_feat)\n",
        "          return future"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, train_loader, val_loader, optimizer, logger, num_epochs=50,lambda_loss=0.1, scheduler=None):\n",
        "    \"\"\"\n",
        "    Training routing with auxiliary loss\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    best_model=None\n",
        "    best_ADE=10e10\n",
        "\n",
        "    criterion1 = nn.MSELoss()\n",
        "    criterion2 = nn.L1Loss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss1 = 0\n",
        "        train_loss2 = 0\n",
        "        train_loss = 0\n",
        "        for idx, batch in enumerate(train_loader):\n",
        "            camera = batch['camera'].to(device)\n",
        "            history = batch['history'].to(device)\n",
        "            future = batch['future'].to(device)\n",
        "            command = batch['command'].to(device)\n",
        "            depth = batch['depth'].to(device)         # ADDED\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            pred_future, pred_depth = model(camera,history,command,depth)\n",
        "            loss1 = criterion1(pred_future[..., :2], future[..., :2])\n",
        "            loss2 = criterion2(pred_depth,depth)\n",
        "            loss=loss1+lambda_loss*loss2\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if idx % 10 == 0:\n",
        "                logger.log(step=epoch * len(train_loader) + idx, loss=loss.item())\n",
        "            train_loss += loss.item()\n",
        "            train_loss1 += loss1.item()\n",
        "            train_loss2 += loss2.item()\n",
        "            \n",
        "        if scheduler!=None:\n",
        "            scheduler.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss,val_loss1,val_loss2, ade_all, fde_all = 0,0,0, [], []\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                camera = batch['camera'].to(device)\n",
        "                history = batch['history'].to(device)\n",
        "                future = batch['future'].to(device)\n",
        "                command = batch['command'].to(device)\n",
        "                depth = batch['depth'].to(device)         # ADDED\n",
        "\n",
        "                pred_future,pred_depth = model(camera, history, command,depth)\n",
        "                loss1 = criterion1(pred_future[..., :2], future[..., :2])\n",
        "                loss2 = criterion2(pred_depth,depth)\n",
        "                loss=loss1+lambda_loss*loss2\n",
        "                ADE = torch.norm(pred_future[:, :, :2] - future[:, :, :2], p=2, dim=-1).mean()\n",
        "                FDE = torch.norm(pred_future[:, -1, :2] - future[:, -1, :2], p=2, dim=-1).mean()\n",
        "                ade_all.append(ADE.item())\n",
        "                fde_all.append(FDE.item())\n",
        "                val_loss += loss.item()\n",
        "                val_loss1 += loss1.item()\n",
        "                val_loss2 += loss2.item()\n",
        "\n",
        "\n",
        "        # Save best model\n",
        "        ADE=np.mean(ade_all)\n",
        "        if ADE<best_ADE:\n",
        "              best_ADE=ADE\n",
        "              best_model=copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss/len(train_loader):.3f}={train_loss1/len(train_loader):.2f}+{lambda_loss*train_loss2/len(train_loader):.2f} | Val Loss: {val_loss/len(val_loader):.3f}={val_loss1/len(val_loader):.2f}+{lambda_loss*val_loss2/len(val_loader):.2f} | ADE: {np.mean(ade_all):.4f} | FDE: {np.mean(fde_all):.4f} | Best ADE: {best_ADE:.4f}')\n",
        "\n",
        "    return best_model,best_ADE\n",
        "\n",
        "\n",
        "def train2(model, train_loader, val_loader, optimizer, logger, num_epochs=50, scheduler=None):\n",
        "    \"\"\"\n",
        "    Training routing without auxiliary loss\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    best_model=None\n",
        "    best_ADE=10e10\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for idx, batch in enumerate(train_loader):\n",
        "            camera = batch['camera'].to(device)\n",
        "            history = batch['history'].to(device)\n",
        "            future = batch['future'].to(device)\n",
        "            command = batch['command'].to(device)\n",
        "            depth = batch['depth'].to(device)         # ADDED\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            pred_future = model(camera,history,command,depth)\n",
        "            loss = criterion(pred_future[..., :2], future[..., :2])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            if idx % 10 == 0:\n",
        "                logger.log(step=epoch * len(train_loader) + idx, loss=loss.item())\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        if scheduler!=None:\n",
        "            scheduler.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss, ade_all, fde_all = 0, [], []\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                camera = batch['camera'].to(device)\n",
        "                history = batch['history'].to(device)\n",
        "                future = batch['future'].to(device)\n",
        "                command = batch['command'].to(device)\n",
        "                depth = batch['depth'].to(device)         # ADDED\n",
        "\n",
        "                pred_future = model(camera, history, command, depth)\n",
        "                loss = criterion(pred_future, future)\n",
        "                ADE = torch.norm(pred_future[:, :, :2] - future[:, :, :2], p=2, dim=-1).mean()\n",
        "                FDE = torch.norm(pred_future[:, -1, :2] - future[:, -1, :2], p=2, dim=-1).mean()\n",
        "                ade_all.append(ADE.item())\n",
        "                fde_all.append(FDE.item())\n",
        "                val_loss += loss.item()\n",
        "\n",
        "\n",
        "        # Save best model\n",
        "        ADE=np.mean(ade_all)\n",
        "        if ADE<best_ADE:\n",
        "              best_ADE=ADE\n",
        "              best_model=copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss/len(train_loader):.4f} | Val Loss: {val_loss/len(val_loader):.4f} | ADE: {np.mean(ade_all):.4f} | FDE: {np.mean(fde_all):.4f} | Best ADE: {best_ADE:.4f}')\n",
        "\n",
        "    return best_model,best_ADE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7HBGrjA8F4D"
      },
      "outputs": [],
      "source": [
        "train_data_dir = \"train\"\n",
        "val_data_dir = \"val\"\n",
        "\n",
        "train_files = [os.path.join(train_data_dir, f) for f in os.listdir(train_data_dir) if f.endswith('.pkl')]\n",
        "val_files = [os.path.join(val_data_dir, f) for f in os.listdir(val_data_dir) if f.endswith('.pkl')]\n",
        "\n",
        "train_dataset = DrivingDataset2(train_files,augment=True)\n",
        "val_dataset = DrivingDataset2(val_files,val=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, num_workers=2, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, num_workers=2)\n",
        "\n",
        "model = DrivingPlanner(camera_dim=1024,history_dim=128,command_dim=32,dropout=0.1,version=3,d_latent=256)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "scheduler = StepLR(optimizer, step_size=40, gamma=0.1)\n",
        "\n",
        "logger = Logger()\n",
        "\n",
        "print('Initialization successful, starting training')\n",
        "\n",
        "best_model,best_ADE=train2(model, train_loader, val_loader, optimizer, logger, scheduler,num_epochs=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since everything was run on Scitas, the output are not available here but in the joined log file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model weights saving and submission generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')\n",
        "torch.save(best_model, \"drive/MyDrive/Colab Notebooks/phase2_best.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_model = DrivingPlanner(camera_dim=1024,history_dim=128,command_dim=32,dropout=0.1,version=3,d_latent=256)\n",
        "final_model.load_state_dict(torch.load(\"drive/MyDrive/Colab Notebooks/phase2_best.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDjhfBrCP7Ia",
        "outputId": "f8e1886e-a3aa-4575-d5f7-c37ca934633f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['camera', 'depth', 'driving_command', 'sdc_history_feature', 'semantic_label'])\n"
          ]
        }
      ],
      "source": [
        "with open(f\"test_public/0.pkl\", \"rb\") as f:\n",
        "    data = pickle.load(f)\n",
        "print(data.keys())\n",
        "# Note the absence of sdc_future_feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-JqhIfNP7Ib",
        "outputId": "c526f181-79e8-49e3-fcda-d9453e683840"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of df_xy: (1000, 121)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "final_model=final_model.to(device)\n",
        "test_data_dir = \"test_public\"\n",
        "test_files = [os.path.join(test_data_dir, fn) for fn in sorted([f for f in os.listdir(test_data_dir) if f.endswith(\".pkl\")], key=lambda fn: int(os.path.splitext(fn)[0]))]\n",
        "test_dataset = DrivingDataset(test_files, test=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=250, num_workers=2)\n",
        "final_model.eval()\n",
        "all_plans = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        camera = batch['camera'].to(device)\n",
        "        history = batch['history'].to(device)\n",
        "        command = batch['command'].to(device)\n",
        "        depth = batch['depth'].to(device)\n",
        "\n",
        "        print(camera.shape)\n",
        "\n",
        "        pred_future = final_model(camera,history,command,depth)\n",
        "        all_plans.append(pred_future.cpu().numpy()[..., :2])\n",
        "all_plans = np.concatenate(all_plans, axis=0)\n",
        "\n",
        "# Now save the plans as a csv file\n",
        "pred_xy = all_plans[..., :2]  # shape: (total_samples, T, 2)\n",
        "\n",
        "# Flatten to (total_samples, T*2)\n",
        "total_samples, T, D = pred_xy.shape\n",
        "pred_xy_flat = pred_xy.reshape(total_samples, T * D)\n",
        "\n",
        "# Build a DataFrame with an ID column\n",
        "ids = np.arange(total_samples)\n",
        "df_xy = pd.DataFrame(pred_xy_flat)\n",
        "df_xy.insert(0, \"id\", ids)\n",
        "\n",
        "# Column names: id, x_1, y_1, x_2, y_2, ..., x_T, y_T\n",
        "new_col_names = [\"id\"]\n",
        "for t in range(1, T + 1):\n",
        "    new_col_names.append(f\"x_{t}\")\n",
        "    new_col_names.append(f\"y_{t}\")\n",
        "df_xy.columns = new_col_names\n",
        "\n",
        "# Save to CSV\n",
        "df_xy.to_csv(\"submission_phase2.csv\", index=False)\n",
        "\n",
        "print(f\"Shape of df_xy: {df_xy.shape}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
